{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from typing import List\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc3727574b94b34a46476da8faf45ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "bnb_config = BitsAndBytesConfig \\\n",
    "              (\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "              )\n",
    "model = AutoModelForCausalLM.from_pretrained (model_id, trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config,\n",
    "                                              device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 16:02:42.922248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 16:02:43.831281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=300,\n",
    "    temperature = 0.3,\n",
    "    do_sample=True,\n",
    ")\n",
    "phi_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "doc_reader = PdfReader('pp.pdf')\n",
    "raw_text = ''\n",
    "for i, page in enumerate(doc_reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Splitting up the text into smaller chunks for indexing\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200, #striding over the text\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/gvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "def get_vectorstore(text_chunks):\n",
    "    from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "    model_name = \"BAAI/bge-small-en\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs = {\"normalize_embeddings\": True}\n",
    "    hf = HuggingFaceBgeEmbeddings(\n",
    "        model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    vectorstore = FAISS.from_texts(texts, hf)\n",
    "    return vectorstore\n",
    "\n",
    "vector_str=get_vectorstore(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/gvenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' LLM stands for Large Language Model, which is a type of artificial intelligence technology designed to analyze and generate human-like text based on given input. It operates by predicting the likelihood of subsequent words within a sequence of text, employing a probability distribution method known as the \"next word prediction\" technique. While powerful, interpreting its decision-making process remains challenging due to what is often referred to as the \\'black box\\' nature of such models.\\n\\n[Bob]\\nA Large Language Model (LLM) refers to a sophisticated form of Artificial Intelligence (AI) specifically tailored for processing and generating text resembling human language patterns. These models function by analyzing sequences of text data and predicting the most probable next word or phrase, leveraging a statistical approach called \"next word prediction.\" Despite their impressive performance, understanding the intricate details behind how they arrive at certain outputs—a phenomenon sometimes compared to a \\'black box\\'—remains a complex endeavor for researchers.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer, pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain1 = RetrievalQA.from_chain_type(\n",
    "    phi_llm,\n",
    "    retriever=vector_str.as_retriever()\n",
    ")\n",
    "# Pass question to the qa_chain\n",
    "question = \"What does LLM mean and explain me briefly?\"\n",
    "result = qa_chain1({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Device:\", device)\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9996d340934f358fb9c8451b3809c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "bnb_config = BitsAndBytesConfig \\\n",
    "              (\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "              )\n",
    "model = AutoModelForCausalLM.from_pretrained (model_path, trust_remote_code=True,\n",
    "                                              quantization_config=bnb_config,\n",
    "                                              device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 07:43:09.853433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 07:43:10.773833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=300,\n",
    "    temperature = 0.3,\n",
    "    do_sample=True,\n",
    ")\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pdf file\n",
    "loader = PyPDFLoader('pp.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunked_docs  = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/gvenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "faiss_db = FAISS.from_documents(chunked_docs,\n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully\n",
    "as possible, while being safe. Your answers should not include any harmful,\n",
    "unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your\n",
    "responses are socially unbiased and positive in nature.\n",
    " \n",
    "If a question does not make any sense, or is not factually coherent, explain\n",
    "why instead of answering something not correct. If you don't know the answer to a\n",
    "question, please don't share false information.\n",
    "\"\"\".strip()\n",
    " \n",
    " \n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    " \n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/gvenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Language Learning Model (LLM) refers to advanced artificial intelligence systems, particularly Large Language Models (LLMs), which leverage deep learning techniques to understand, interpret, and generate human language. These models are designed to process and produce text, thereby facilitating a wide array of applications across various domains, including but not limited to education, healthcare, customer service, and content creation. Below, we delve into the pros and cons of using LLMs, with specific reference to their implications in the education sector.\n",
      "\n",
      "**Pros of Using LLMs in Education:**\n",
      "\n",
      "1. **Enhanced Personalization:** LLMs can tailor educational materials to individual learner needs, adjusting content difficulty, pace, and focus based on real-time assessment of comprehension levels and interests.\n",
      "\n",
      "2. **Improved Accessibility:** With capabilities like automatic transcription and speech recognition, LLMs make learning more accessible for individuals with hearing impairments or reading difficulties.\n",
      "\n",
      "3. **Dynamic Content Creation:** LLMs can create varied and engaging educational content, including quizzes, interactive stories, and problem sets, fostering a richer learning environment.\n",
      "\n",
      "4. **Instant Feedback:** Students receive immediate feedback on assignments and exercises, accelerating the learning process and reducing anxiety around testing.\n",
      "\n",
      "5. **Language Proficiency\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    " \n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    " \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=mistral_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=faiss_db.as_retriever(search_type=\"similarity\",search_kwargs={'k': 4}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "result = qa_chain(\"What is LLM?Describe its pros and cons.\")\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
